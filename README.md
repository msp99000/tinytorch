# ğŸ§  TinyTorch: Lightweight LLM Optimization Pipeline

**TinyTorch** is a proof-of-concept (PoC) pipeline that implements advanced compression techniques for large language models (LLMs), enabling their efficient deployment on constrained environments â€” including edge devices like Jetson Nano.

> ğŸ”¬ *Built with pruning, knowledge distillation, and QLoRA quantization techniques.*

---

## ğŸš€ Objectives

The goal of this project is to demonstrate:
- âœ… Structural model pruning for reducing parameter count
- âœ… Knowledge distillation to transfer knowledge from large models to compact ones
- âœ… QLoRA quantization for low-bit precision inference (e.g., 4-bit)
- âœ… Potential deployment on low-resource or edge environments (Jetson, Raspberry Pi, etc.)

---

## ğŸ“Œ Technologies Used

| Component         | Tool/Library                  |
|------------------|-------------------------------|
| ğŸ§  Base Models    | Hugging Face Transformers     |
| ğŸ§ª Distillation   | PyTorch + Hugging Face Trainer|
| âœ‚ï¸ Pruning        | `torch.nn.utils.prune`        |
| ğŸ§® Quantization   | `bitsandbytes`, `AutoGPTQ`, `QLoRA` |
| ğŸ“Š Tracking       | Weights & Biases (optional)   |
| ğŸ” Datasets       | SST-2, IMDb, AG News          |
| ğŸ–¥ï¸ Infra          | Google Colab / Kaggle GPUs    |

---

## ğŸ“ˆ Pipeline Overview

```text
[ Base Model ] â”€â”€â–¶ [ Pruning ] â”€â”€â–¶ [ Distillation ] â”€â”€â–¶ [ QLoRA (4-bit) ] â”€â”€â–¶ [ Inference on Edge ]
       â”‚                                          â”‚
       â”‚                                          â””â”€â”€â”€â”€â”€â–¶ Student Model (compressed)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Evaluation & Comparison
